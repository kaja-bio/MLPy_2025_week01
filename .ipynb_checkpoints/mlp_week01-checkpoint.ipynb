{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f78c140",
   "metadata": {},
   "source": [
    "# Week 01 - Data Pre-Processing\n",
    "\n",
    "## Aims\n",
    "\n",
    "By the end of this notebook you will \n",
    "\n",
    "* understand and play with the different aspects of data pre-processing\n",
    "* be familiar with tools for exploratory data analysis and visualization\n",
    "* understand the basics of feature engineering\n",
    "\n",
    "## Topics and Instructions\n",
    "\n",
    "1. [Problem Definition and Setup](#setup)\n",
    "\n",
    "2. [Exploratory Data Analysis](#eda)\n",
    "\n",
    "3. [Data Preprocessing](#prep)\n",
    "\n",
    "4. [Feature Engineering](#engin)\n",
    "\n",
    "5. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8862515",
   "metadata": {},
   "source": [
    "In lecture this week, we reviewed the general **machine learning pipline**, which following the [\"Machine Learning Project Checklist\"](https://github.com/ageron/handson-ml/blob/master/ml-project-checklist.md) of Geron (2019) can be stuctured as:\n",
    "\n",
    ">- Frame the problem and look at the big picture.\n",
    ">- Get the data.\n",
    ">- Explore the data and gain insights.\n",
    ">- Prepare the data to better expose the underlying data patterns to Machine Learning algorithms.\n",
    ">- Explore many different models and shortlist the best ones.\n",
    ">- Fine-tune your models and combine them into a great solution.\n",
    ">- Present your solution.\n",
    ">- Launch, monitor, and mantain your system.\n",
    "\n",
    "In this week's workshop, we will focus on the initial steps of this pipeline, that is on, data pre-processing, exploratory data analysis and feature engineering.\n",
    "\n",
    "During workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. During the first few weeks, the worksheets will contain cues to switch roles between driver and navigator. When completing worksheets:\n",
    "\n",
    ">- You will have tasks tagged by (CORE) and (EXTRA). \n",
    ">- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n",
    ">- Look for the 🏁 as cue to switch roles between driver and navigator.\n",
    ">- In some Exercises, you will see some beneficial hints at the bottom of questions.\n",
    "\n",
    "Instructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0c3c9",
   "metadata": {},
   "source": [
    "# Problem Definition and Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d830b3",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "Now lets load in some packages to get us started. The following are widely used libraries to start working with Python in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fba9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd4cc0",
   "metadata": {},
   "source": [
    "If you need to install any packages from scratch, you need to install the related library before calling it. For instance, [feature-engine](https://feature-engine.trainindata.com/en/latest/) is a Python library for Feature Engineering and Selection, which: \n",
    "\n",
    "- contains multiple transformers to engineer and select features to use in machine learning models.\n",
    "\n",
    "- preserves scikit-learn functionality with methods fit() and transform() to learn parameters from and then transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the feature-engine library (if not already installed)\n",
    "# !pip install feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca0145",
   "metadata": {},
   "source": [
    "In some cases, we may need only a component of the whole library. If this is the case, it is possible to import specific things from a module (library), using the following line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import DropMissingData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400191a",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Now, it is time move on to the next step.\n",
    "\n",
    "> Welcome to Machine Learning Housing Corporation! The first task you are asked to perform is to build a model of housing prices in California using the California census data. This data has metrics such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them “districts” for short.\n",
    ">\n",
    "> **Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.**\n",
    ">\n",
    "> The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. **How does the company expect to use and benefit from this model?** This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.\n",
    ">\n",
    "> The next question to ask is **what the current solution looks like (if any)**. It will often give you a reference performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up-to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\n",
    ">\n",
    "> This is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%. This is why the company thinks that it would be useful to train a model to predict a district’s median housing price given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0831296-b285-4fcd-a4bd-c650afc17e80",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 1 (CORE)\n",
    "\n",
    "Using the information above answer the following questions about how you may design your machine learning system.\n",
    "\n",
    "a) Is this a supervised or unsupervised learning task? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0969486-871f-451e-95ac-5fda59b50e75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a490f1-f01c-45ad-b5db-2012f5d90b5f",
   "metadata": {},
   "source": [
    "b) Is this a classification, regression, or some other task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293a886-4353-479f-a1b6-b75b42c087ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "980d7283-8c7b-4f97-89fc-3585b51fa53a",
   "metadata": {},
   "source": [
    "c) Suppose you are only required to predict if a district's median housing prices are \"cheap,\" \"medium,\" or \"expensive\". Will this be the same or a different task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840905cc-cb04-4b48-9663-0428edf03a86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2d6efb",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "\n",
    "The data we will be using this week is a modified version of the California Housing dataset. We can get the data a number of ways. The easiest is just to load it from the working directory that we are working on (where we have already downloaded it to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90236e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a744a3c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "In this section we are going to start with exploring the California Housing data using methods that you will likely already be familiar with.\n",
    "\n",
    "Data can come in a broad range of forms encompassing a collection of discrete objects, numbers, words, events, facts, measurements, observations, or even descriptions of things. Processing data using exploratory data analysis (EDA) can elicit useful information and knowledge by examining the available dataset to discover patterns, spot anomalies, test hypotheses, and check assumptions. \n",
    "\n",
    "Let's start by examining the [Data Dictionary](https://www.kaggle.com/camnugent/california-housing-prices) and the variables available:\n",
    "\n",
    "> `longitude`: A measure of how far west a house is; a higher value is farther west\n",
    ">\n",
    "> `latitude`: A measure of how far north a house is; a higher value is farther north\n",
    ">\n",
    "> `housingMedianAge`: Median age of a house within a block; a lower number is a newer building\n",
    ">\n",
    "> `totalRooms`: Total number of rooms within a block\n",
    ">\n",
    "> `totalBedrooms`: Total number of bedrooms within a block\n",
    ">\n",
    "> `population`: Total number of people residing within a block\n",
    ">\n",
    "> `households`: Total number of households, a group of people residing within a home unit, for a block\n",
    ">\n",
    "> `medianIncome`: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
    ">\n",
    "> `medianHouseValue`: Median house value for households within a block (measured in US Dollars)\n",
    ">\n",
    "> `oceanProximity`: Location of the house w.r.t ocean/sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa49b60",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 2 (CORE)\n",
    "\n",
    "a) Examine the datatypes for each column calling [`info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html). What is the total number of observations and total number of variables? What is the type of each variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a799c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0673d936",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8379e17f",
   "metadata": {},
   "source": [
    "b) From the information provided above, can you anticipate any data cleaning we may need to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64363932",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7857d21",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 3  (CORE)\n",
    "\n",
    "a) Use descriptive statistics and histograms to examine the distributions of the numerical attributes.\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- <code>.describe()</code> can be used to create summary descriptive statistics on a pandas dataframe.\n",
    "- You can use a [<code>sns.histplot</code>](https://seaborn.pydata.org/generated/seaborn.histplot.html) to create histograms\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cffb13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b64ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7393857",
   "metadata": {},
   "source": [
    "b) Can you identify other pre-processing/feature engineering steps we may need to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c9011",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d38752",
   "metadata": {},
   "source": [
    "c) One thing you may have noticed from the histogram is that the median income, housing median age, and the median house value are capped. The median house value capping (this being our target value) may or may not be a problem depending on your client. If we needed precise predictions beyond $\\$500,000$, we may need to either collect proper labels/outputs for the districts whose labels were capped or remove these districts from the data. Following the latter, remove all districts whose median house value is capped. How many observations are there now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47610e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the cases where median_house_value >= 500,000$\n",
    "housing = housing[housing[\"median_house_value\"] < 500000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aba6db",
   "metadata": {},
   "source": [
    "\n",
    "### 🚩 Exercise 4 (CORE)\n",
    "\n",
    "What are the possible categories for the `ocean_proximity` variable? Are the number of instances in each category similar? \n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- [<code>value_counts()</code>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.value_counts.html) can be used to count the values of the categories a pandas series.\n",
    "- You can use a [<code>sns.countplot</code>](https://seaborn.pydata.org/generated/seaborn.countplot.html) to create barplot with the number of instances of each category \n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f27705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466f4d66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8db2956a",
   "metadata": {},
   "source": [
    "🏁 **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcef64",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 5 (CORE)\n",
    "\n",
    "Examine if/which of the features are correlated to each other. Are any of the features correlated with our output (`median_house_value`) variable?\n",
    "\n",
    "- Can you think of any reason why certain features may be correlated?\n",
    "\n",
    "- How might we use this information in later steps of our model pipeline?\n",
    "\n",
    "<br><br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "    \n",
    "- <code>.corr()</code> can be used to compute the correlations.\n",
    "- You can use a [<code>sns.heatmap</code>](https://seaborn.pydata.org/generated/seaborn.heatmap.html) to visualize the correlations\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5802979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d5c518",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d96a23f",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 6 (CORE)\n",
    "\n",
    "Use `sns.pairplot` to further investigate the joint relationship between each pair of variables. What insights into the data might this provide over looking only at the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ea6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1b83b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24de6ef",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 7 (CORE)\n",
    "\n",
    "Which variables in our data represent counts and how are they distributed? Use a probability plot `scipy.stats.probplot(df[variable], dist=\"norm\")` to compare their distributions with a normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057ac67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aac5aefc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99f2e060",
   "metadata": {},
   "source": [
    "# Data Pre-Processing <a id='prep'></a>\n",
    "\n",
    "Now we have some familiarity with the data though EDA, lets start preparing our data to be modelled.\n",
    "\n",
    "From here on in, we should really be creating functions for our data transformations. This is because when we want to run data through our \"model pipeline\" in the future, rather than having to copy and paste a load of code, we can just use a series of functions. Later on the course, we will see this is useful when we split our data into training, validation, and test sets, but this would also be required if you deploy your model in a \"live\" environment. Furthermore, when refining a model it makes it easier for us to treat our preparation choices as \"hyperparameters\", meaning we can easily add or remove parts of our pipeline to see what works and what doesn't.\n",
    "\n",
    "Its also worth examining what is meant by a **\"Pipeline\"**. \n",
    "\n",
    "- A general definition is that it is just a sequence of data preparation operations that is ensured to be reproducible. \n",
    "\n",
    "- However, we may want to ensure that any functions/classes we make for our pipeline have specific attributes that work best with the tools available in our chosen machine learning library. \n",
    "\n",
    "- In this course we are mostly going to be using `Scikit-learn`, with a little `Keras` at the end for neural networks.\n",
    "\n",
    "__Scikit-learn__\n",
    "\n",
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning (https://scikit-learn.org/stable/getting_started.html). \n",
    "\n",
    "It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. \n",
    "\n",
    "In Scikit-Learn a `Pipeline` is a class we can use to combine our pre-processing and modelling steps together (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "A `Pipeline` is useful for many reasons, including that they help prevent you from data leakage, i.e. disclosing some testing data in your training data. Objects that go into a scikit-learn `Pipeline` can either be _transformer_ or _estimator_ classes, or, if we use an imbalanced-learn `Pipeline` instead, also _resamplers_.\n",
    "\n",
    "All three of these objects (_resamplers_, _transformers_, and _estimator_) all typically have a `.fit()` method, which is used to\n",
    "- validate and interpret any parameters, \n",
    "- validate the input data, \n",
    "- estimate and store attributes from the parameters and provided data, \n",
    "- return the fitted estimator to facilitate method chaining in a pipeline. \n",
    "\n",
    "Along with other sample properties (e.g. `sample_weight`), the `.fit()` method usually takes two inputs:\n",
    "\n",
    "> - The input matrix (or design matrix) X. The size of X is typically (n_samples, n_features), which means that samples are represented as rows and features are represented as columns.\n",
    ">\n",
    "> - The target values y which are real numbers for regression tasks, or integers for classification (or any other discrete set of values). For unsupervized learning tasks, y does not need to be specified. \n",
    ">\n",
    "> https://scikit-learn.org/stable/getting_started.html\n",
    "\n",
    "Other methods available for these objects other than `.fit()` will depend on what they are, so we will look at each of these objects in turn and then combine them into a model pipeline later in this workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba6db4",
   "metadata": {},
   "source": [
    "In general, it is good practice to adhere the **Scikit-Learn Design principles** when creating our pipeline steps.\n",
    "\n",
    ">- __Nonproliferation of classes__. Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.\n",
    ">- This means all our functions should **input and output Numpy arrays**. You can of course use pandas functions if you wish in your function, but just make sure that you convert the data back to output the Numpy array object! \n",
    ">- Otherwise, in some steps, there might appear some object/data type errors\n",
    "\n",
    "Lets start by splitting our **features** from our **target** variable in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features from the data\n",
    "X = housing.drop(\"median_house_value\", axis = 1)\n",
    "features = list(X.columns)\n",
    "print(features)\n",
    "\n",
    "X = X.values\n",
    "print(X) # Or display(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeebd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target features from the data\n",
    "y = housing[\"median_house_value\"].copy()\n",
    "y = y.values\n",
    "\n",
    "print(y) # Or display(y_train)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f35fde",
   "metadata": {},
   "source": [
    "\n",
    "## Data Cleaning\n",
    "\n",
    "At this stage in our workflow we may want to deal with duplicated/missing values and (optionally) fix/remove outliers. Let's start with the former.\n",
    "\n",
    "We want to remove duplicates as they may bias our fitted model. In other words, we may potentially *overfit* to this subset of points. However, care should usually be taken to check they are not _real_ data with identical values.\n",
    "\n",
    "There a number of ways we could identify duplicates, the simplist (and the approach we'll focus on) is just to find observations with the same feature values. Of course this will not identify things such as spelling errors, missing values, address changes, use of aliases, etc. For those such things, more complicated methods along with manual assessment is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81217fbc",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 8 (CORE)\n",
    "\n",
    "Are there any duplicated values in the data? If so how many?\n",
    "\n",
    "<br>\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "\n",
    "With Pandas dataframes you can use `.duplicated()` to get a boolean of whether something is a duplicate and then use `.sum()` to count how many there are.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be94a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beefda73",
   "metadata": {},
   "source": [
    "🏁 **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0440d0",
   "metadata": {},
   "source": [
    "### Outlier Detection\n",
    "\n",
    "An **Outlier** is a data point that lies abnormally far from other observations and may distort the model fit and results. In general, it is a good idea to examine if any outliers are present during preprocessing.  However, it may not always be appropriate to drop these observations without explicit knowledge and testing if they are really outliers or not.\n",
    "\n",
    "We will use basic statistics in order to try to identify outliers. A simple method of detecting outliers is to use the **inter-quartile range (IQR) proximity rule** (Tukey fences) which states that a value is an outlier if it falls outside these boundaries:\n",
    "\n",
    "- Upper boundary = 75th quantile + (IQR * $k$) \n",
    "\n",
    "- Lower boundary = 25th quantile - (IQR * $k$)\n",
    "\n",
    "where IQR = 75th quantile - 25th quantile (the length of the box in the box plot). Here, $k$ is a nonnegative constant which is typically set to 1.5. However, it is also common practice to find extreme values by setting $k$ to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e01d4e",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 9 (EXTRA)\n",
    "\n",
    "Can you identify any potential outliers using the generated boxplots below ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116d9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (20,10), ncols = X.shape[-1]//2, nrows = 2, sharex = True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.boxplot(y = X[:,i], ax = ax) \n",
    "    ax.set_title(features[i])\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "plt.suptitle(\"Boxplots\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1461ae4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "528f5297",
   "metadata": {},
   "source": [
    "- After detecting outliers we may want to set them to NA values so we can use subseqent methods, e.g to remove them (NA Removal). \n",
    "\n",
    "- In general, it is a good idea to make our methods into functions that are compatible with scikit-learn, so that we can use them in a `Pipeline` later on. \n",
    "\n",
    "- The removal of outliers is NOT an easy task, should not be applied all the time, especially if you are interested in rare events. In general, without any specific knowledge for the problem at hand or without statistical examination of outliers via suitable testing tools, it is not recommended to remove these observations by looking only at the above boxplots. \n",
    "\n",
    "- Another issue might be related to the sample size of your data set. Even though the `housing` data is large, in some cases, when you drop those observations you can discard important information unwittingly!\n",
    "\n",
    "**For the sake of simplicity, we are keeping all observations at that moment for this workshop.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a1f5d",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "Most ML models cannot handle missing values, and as we saw earlier, there are some present in `total_bedrooms`. \n",
    "\n",
    "There are a number of ways we can deal with missing values but to start with, let's just **remove NA values**. \n",
    "We can do this in two ways by either:\n",
    "\n",
    "1. Getting rid of the corresponding observations (deleting the corresponding rows).\n",
    "2. Getting rid of the whole attribute (deleting the corresponding columns).\n",
    "\n",
    "To do this outside of a pipeline is relatively straight forward, we just run `housing.dropna()` with either the `axis` set to `0` or `1` (depending if we want to remove rows or columns) before splitting our data into features (`X`) and outputs (`y`). \n",
    "\n",
    "However if we want to do this in a pipeline we can use the _transformer_ [`DropMissingData`](https://feature-engine.trainindata.com/en/1.8.x/api_doc/imputation/DropMissingData.html) from `feature_engine.imputation`. Instead of simply dropping missing data, we may instead want to use other imputation methods are available in [`sklearn.impute`](https://scikit-learn.org/1.5/api/sklearn.impute.html). \n",
    "\n",
    "#### **Transfomers**\n",
    "\n",
    "If we want to __alter the features__ of our data using a function that is compatible with a `Pipeline`, we need to a _transfomer_.\n",
    "\n",
    "- Transformers are classes that follow the scikit-learn API in Scikit-Learn [clean](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing), [impute](https://scikit-learn.org/1.5/modules/impute.html), [reduce](https://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction), [expand](https://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation), or [generate](https://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction) feature representations.\n",
    "\n",
    "- Transformers are classes with a `.fit()` method, which learn model parameters (e.g. mean for mean imputation) from a training set, and a `.transform()` method which applies this transformation model to data. To create a custom transformer, all you need is to create a class that implements three methods: `fit()`, `transform()`, and `fit_transform()`.\n",
    "\n",
    "Therefore to transform a dataset, each sampler implements:\n",
    "\n",
    "```\n",
    "obj.fit(data)\n",
    "data_transformed = obj.transform(data)\n",
    "```\n",
    "\n",
    "or simply...\n",
    "\n",
    "```\n",
    "data_transformed = obj.fit_transform(data)`\n",
    "```\n",
    "\n",
    "See more details: https://scikit-learn.org/stable/data_transforms.html. In the following subsections, we will see examples of _transformers_ for categorical and numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e85909",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 10  (CORE)\n",
    "\n",
    "Use `dropMissingData` to remove the missing observations from `X`. What is the shape after dropping the missing observations?\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- It may be tempting to overwrite `X` while working on our pre-processing steps. __Don't do this!__ We will run these objects through these steps inside our pipeline later so if you want to test your function make sure to assign the output to tempory objects (e.g. `X_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebd874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import DropMissingData\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b1a45",
   "metadata": {},
   "source": [
    "Note that later in the course, we will also learn about  **Resamplers** which allow us to _alter the number observations_ in our data using a function that is compatible with a `Pipeline`. For more details: https://imbalanced-learn.org/stable/introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a1d38",
   "metadata": {},
   "source": [
    "#### Data Imputation\n",
    "\n",
    "Instead of removing the missing data we can set it to some value. To do this, Scikit-Learn gives us a handy [`SimpleImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.SimpleImputer.html) method which provides simple strategies (e.g. `\"mean\"`, `\"median\"` for numerical features and `\"most_frequent\"` for categorical features). You can also add a missing indicator, which may be useful in the case when missing features may be provide information for predicting the target (e.g. obese patients may prefer not to report bmi, thus, this missingness could be useful for estimating the risk of health conditions or diseases). Beyond simple imputation strategies, sklearn also provides more advanced imputation strategies in [`IterativeImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) and [`KNNImputer`](https://scikit-learn.org/1.5/modules/generated/sklearn.impute.KNNImputer.html). Other strategies are also available in `feature_engine.imputation`: https://feature-engine.trainindata.com/en/1.8.x/user_guide/imputation/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "X_imp = num_imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55245b",
   "metadata": {},
   "source": [
    "When we applied this to our data, we get the following error:\n",
    "\n",
    "```\n",
    "ValueError: Cannot use median strategy with non-numeric data:\n",
    "could not convert string to float:\n",
    "```\n",
    "\n",
    "This is because the \"median\" strategy can only be used with numerical attributes so we need a way of only applying imputation to certain attributes. As with most things in Python, there are a number of options available to us (you may have already used one of them in answers to previous questions): \n",
    "\n",
    "- We could temporarily remove the categorical feature from our data to apply our function\n",
    "```\n",
    "X_num = X[:,:-1]\n",
    "X_num = num_imputer.fit_transform(X_num)\n",
    "```\n",
    "- Apply the function to a subset of the data and assign the output to the same subset \n",
    "```\n",
    "X[:,:-1] = num_imputer.fit_transform(X[:,:-1])\n",
    "```\n",
    "\n",
    "However scikit-learn has a handy function to specify what column we want to apply a function to (its also compatible with `Pipeline`s as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_cols_imputer = ColumnTransformer(\n",
    "    # apply the `num_imputer` to all columns apart from the last\n",
    "    [(\"num\", num_imputer, list(range(X.shape[1] - 1)))],\n",
    "    # don't touch all other columns, instead concatenate it on the end of the\n",
    "    # changed data.\n",
    "    remainder = \"passthrough\"\n",
    ") \n",
    "\n",
    "X_ = num_cols_imputer.fit_transform(X)\n",
    "print(\"Number of Missing Values\")\n",
    "pd.DataFrame(X_, columns = features).isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1b205",
   "metadata": {},
   "source": [
    "After using `.fit` we can see that this method stores the median value for each attribute on the training set. This value can be used when validating and testing the model as it will be used if there is missing data in the new data.\n",
    "\n",
    "__Note__\n",
    "- To access the fitted `num_imputer` in this case, you first needed to subset the `transformers_` in `ColumnTransformer` to only the first tuple (`[0]`), and then get the second object (`[1]`) in the list (the `num_imputer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(num_cols_imputer.transformers_[0][1].statistics_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6726df",
   "metadata": {},
   "source": [
    "🏁 **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9872550",
   "metadata": {},
   "source": [
    "# Feature Engineering <a id='engin'></a>\n",
    "\n",
    "As discussed in the lectures, feature engineering is where we extract features from data and transform them into formats that are suitable for machine learning models. Today, we will have a look at two main cases that are present in our data: **categorical** and **numerical** values.\n",
    "\n",
    "Feature engineering also requires a _transformer_ class to __alter the features__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db944478",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "- In the dataset, we have an text attribute (`ocean_proximity`) that we already had to treat differently when cleaning the data. This extends to feature engineering as well, where we need to use separate methods than those used with numerical variables.\n",
    "\n",
    "- If we look at the unique values of this attribute, we will see that there are a limited number of possible values which represent a category. We need a way of encoding this information into our modeling framework **by converting our string/categorical variable into a numeric representation** that can be included in our models.\n",
    "\n",
    "If we have a binary categorical variable (two levels) we could do this by picking one of the categorical levels and encode it as 1 and the other level as 0. \n",
    "\n",
    "However, in this case as we have multiple categories, we would probably want to use another encoding method. To illustrate, we can try encoding the the categorical feature `ocean_proximity` using both the `OrdinalEncoder` and `OneHotEncoder` available in `sklearn.preprocessing`.\n",
    "\n",
    "__Side Notes__\n",
    "\n",
    "- The output of the `OneHotEncoder` provided in Scikit-Learn is a SciPy _sparse matrix_, instead of a NumPy array. These are useful when you have lots of categories as your matrix becomes mostly full of 0's. To store all these 0's takes up unneccesary memory, so instead a sparse matrix just stores the location of nonzero elements. The good news is that you can use a sparse matrix similar to a numpy matrix, but if you wanted to, you can convert it to a dense numpy matrix using `.toarray()`.\n",
    "\n",
    "- The above does not seem to be the case if passed through a `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# Defining the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    # pass through all columns with numerical values \n",
    "    (\"num\", \"passthrough\", list(range(X.shape[1] - 1))), \n",
    "    # apply the ordinal_encoder to the last column\n",
    "    (\"cat\", ordinal_encoder, [X.shape[1] - 1]),\n",
    "])\n",
    "\n",
    "# fitting the encoder defined above\n",
    "housing_cat_encoded = encoder.fit_transform(X) \n",
    "\n",
    "display(dict(zip(list(encoder.transformers_[1][1].categories_[0]), range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a574797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Defining the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "encoder = ColumnTransformer([\n",
    "    # pass through all columns with numerical values \n",
    "    (\"num\", \"passthrough\", list(range(X.shape[1] - 1))), \n",
    "    # apply the onehot_encoder to the last column\n",
    "    (\"cat\", onehot_encoder, [X.shape[1] - 1]),\n",
    "]) \n",
    "\n",
    "housing_cat_1hot = encoder.fit_transform(X) \n",
    "cats = pd.Series(X[:,-1]).unique()\n",
    "pd.DataFrame(encoder.transformers_[1][1].transform(cats.reshape(-1, 1)).toarray(), columns = cats).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477773a9",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 11 (CORE)\n",
    "\n",
    "- What is the main difference between two methods regarding the obtained features ?\n",
    "\n",
    "- Which encoding method do you think is most appropriate for this variable and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02c9e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d36555",
   "metadata": {},
   "source": [
    "Another handy feature of `OneHotEncoder` and `OrdinalEncoder` is that infrequent categories can be aggregated into a single feature/value. The parameters to enable the gathering of infrequent categories are `min_frequency` and `max_categories`.\n",
    "\n",
    "When there are many unordered categories, another useful encoding scheme is [`TargetEncoder`](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder) which uses the target mean conditioned on the categorical feature for encoding unordered categories. Whereas one-hot encoding would greatly inflate the feature space if there are a very large number of categories (e.g. zip code or region), `TargetEncoder` is more parsimonious. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bed85a",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 12 (EXTRA)\n",
    "\n",
    "Use the `max_categories` attribute to set the maximum number of categories to 4. Use the `get_feature_names_out()` method of `OneHotEncoder` to print the new category names. Which two features have been combined?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b4f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ddcf0b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "043c0929",
   "metadata": {},
   "source": [
    "## Numerical Variables\n",
    "\n",
    "### Box-Cox Transformation\n",
    "\n",
    "In some cases, we may wish to apply transformations to our data, so that they have a more Gaussian distribution. For example, log transformations are useful for altering count data to have a more normal distribution as they pull in the more extreme high values relative to the median, while stretching back extreme low values away from the median. You can use a log transformation with either the pre-made `LogTransformer()` from `feature_engine.transformation`, or a custom function and `sklearn.preprocessing.FunctionTransformer`.\n",
    "\n",
    "More generally, the natural logarithm, square root, and inverse transformations are special cases of the **Box-Cox** family of transformations (Box and Cox 1964). The question is why do we need such a transformation and when? \n",
    "\n",
    "- Note that, the method is typically used to transform the outcome, but can also be used to transform predictors. \n",
    "\n",
    "- The method assumes that the variable takes only positive values. If there are any zero or negative values, a modification of the Box-Cox that allows non-positive values (Box-Cox with negatives, BCN) can be used (Hawkins and Weisberg 2017). \n",
    "\n",
    "- If there are zero values, but no negative values, an option is to add a small number (e.g. 1) before carrying out the transformation, although if there are a large number of zero values then it is possible no Box-Cox transformation will sufficiently normalize the variable.\n",
    "\n",
    "- In general, transormations can make interpretations more difficult, thus you should think carefully if they are needed, particullarly if they only result in modest improvements in model performance. Moreover, finding a suitable transformation is typically a trial-and-error process.  \n",
    "\n",
    "Originally, the Box-Cox procedure was developed to transform the outcome and uses maximum likelihood estimation to estimate a transformation parameter $\\lambda$.\n",
    "\n",
    "$\\tilde{y} = \\left\\{ \\begin{array}{l l} \\frac{y^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0 \\\\ \\log y, & \\lambda = 0 \\\\ \\end{array} \\right.$\n",
    "\n",
    "- $\\lambda$ is estimated from the data. \n",
    "\n",
    "- Because the parameter of interest is in the exponent, this type of transformation is called a power transformation\n",
    "\n",
    "- Some values of $\\lambda$ map to common transformations, such as;\n",
    "\n",
    "    >* $\\lambda = 1$ (no transformation)   \n",
    "    >* $\\lambda = 0$ (log)   \n",
    "    >* $\\lambda = 0.5$ (square root)   \n",
    "    >* $\\lambda = -1$ (inverse)\n",
    "\n",
    "- Using the code below, if `lmbda=None` then the function will \"find the lambda that maximizes the log-likelihood function and return it as the second output argument\"\n",
    "\n",
    "- Notice that we can not use `lambda` directly since it conflicts with the available object called `lambda`, this is the reason we preferred the indicator name as `lmbda`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (15,5), ncols = 4, nrows=2, sharey = True)\n",
    "axes = axes.flatten()\n",
    "sns.histplot(data = X[:,6], ax = axes[0])\n",
    "axes[0].set_title(\"Raw Counts\")\n",
    "\n",
    "for i, lmbda in enumerate([0, 0.25, 0.5, 0.75, 1., 1.25, 1.5]):\n",
    "    \n",
    "    house_box_ = stats.boxcox(X[:, 6].astype(float), lmbda = lmbda)\n",
    "    sns.histplot(data = house_box_, ax = axes[i + 1])\n",
    "    axes[i + 1].set_title(\"$\\lambda$ = {}\".format(lmbda))\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5b771",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 13 (EXTRA)\n",
    "\n",
    "- For the variable `households`, based on the `boxcox` transform shown above, which values of $\\lambda$ may be useful? \n",
    "\n",
    "- Apply a similar code snippet to `median_house_value`. Which values of $\\lambda$ may be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9fd8e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052b91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d066d63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a960a4e9",
   "metadata": {},
   "source": [
    "## Other feature types\n",
    "\n",
    "Feature engineering for other feature types beyond numerical categorical are also available in sklearn (e.g. for [text and images](https://scikit-learn.org/1.5/api/sklearn.feature_extraction.html)) and feature engine (e.g. for [Datetime](https://feature-engine.trainindata.com/en/1.8.x/user_guide/datetime/index.html) and for [time series](https://feature-engine.trainindata.com/en/1.8.x/user_guide/timeseries/index.html)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0bc68d",
   "metadata": {},
   "source": [
    "## Feature Combinations\n",
    "\n",
    "- Looking at the datas attributes we may also want to manually combine them into features that are either a more meaningful representation of the data or have better properties.\n",
    "\n",
    "- For example, we know **the number of rooms** in a district, but this may be more useful to combine with the **number of households** so that we have **a measure of rooms per household**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = \"total_rooms\", \"households\"\n",
    "rooms_ix, households_ix = [housing.columns.get_loc(c) for c in col_names] \n",
    "\n",
    "rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "rooms_per_household"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2260f36a",
   "metadata": {},
   "source": [
    "\n",
    "### 🚩 Exercise 14 (EXTRA)\n",
    "\n",
    "- Can you think of other combinations that may be useful?\n",
    "\n",
    "- Create a custom transformer that creates these new combinations of features using the `FunctionTransformer`.\n",
    "\n",
    "<br />\n",
    "<details><summary><b><u>Hint</b></u></summary>\n",
    "What about the following?\n",
    "   \n",
    "- `population_per_household`\n",
    "    \n",
    "- `bedrooms_per_room`\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdb6b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68d54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabc1c5b",
   "metadata": {},
   "source": [
    "🏁 **Now, is a good point to switch driver and navigator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2164de",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "As we will discuss in later weeks, many machine learning algorithms are sensitive to the scale and magnitude of the features, and especially differences in scales across features. For these algorithms, feature scaling will improve performance. \n",
    "\n",
    "For example, let's investigate the range of the features in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48606ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "plt.boxplot(X[:,:-1], vert = False) \n",
    "plt.xscale(\"symlog\") \n",
    "plt.ylabel(\"Feature\") \n",
    "plt.xlabel(\"Feature Range\")\n",
    "\n",
    "ax.set_yticklabels(features[:-1])\n",
    "\n",
    "plt.suptitle(\"Feature Range for the Training Set\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb89ff1",
   "metadata": {},
   "source": [
    "There are various options in scikit learn for feature scaling: \n",
    "\n",
    "- Standardization (`preprocessing.StandardScaler`)\n",
    "\n",
    "- Min-Max Scaling (`preprocessing.MinMaxScaler`)\n",
    "\n",
    "- l2 Normalization (`preprocessing.normalize`)\n",
    "\n",
    "- RobustScaler(`preprocessing.RobustScaler`)\n",
    "\n",
    "- Scale with maximum absolute value (`preprocessing.MaxAbsScaler`)\n",
    "\n",
    ">- As scaling generally improves the performance of most models when features cover a range of scales, it is probably a good idea to apply some sort of scaling to our data before fitting a model. \n",
    ">- *Standardization* (or *variance scaling*), is the most common, but there are a number of other types, as listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cf3cb",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 15 (EXTRA)\n",
    "\n",
    "Try implementing different scalers for the `total_rooms` variable to see the main differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6a3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bd9db96",
   "metadata": {},
   "source": [
    "## Combining into a Pipeline\n",
    "\n",
    "Due to the focus of this week, lets look at combining different feature engineering steps together to make different model pipelines.\n",
    "\n",
    "- We want to create a basic model pipeline that treats the **numerical** and **categorical** attributes differently. \n",
    "- To achive this we are going to use a combination of `Pipeline` and `ColumnTransformer` objects.\n",
    "- We also need to supply the pipeline with a model. For now let's use a linear regression model, which we will learn in more details in week 4.\n",
    "\n",
    "We can of course do this outside of a pipeline, but remember to be careful about data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7744d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "numcols = list(range(X.shape[1]-1))\n",
    "catcols = [X.shape[1]-1]\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_impute\", SimpleImputer(strategy=\"constant\")),\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "reg_pipe_1 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([(\"num_pre\", num_pre, numcols),\n",
    "                                          (\"cat_pre\", cat_pre, catcols)])),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Alternative and equivalent model avoiding nested pipelines\n",
    "# reg_pipe_1 = Pipeline([\n",
    "#     (\"impute\", ColumnTransformer([(\"num_imp\", SimpleImputer(strategy=\"median\"), numcols),\n",
    "#                                           (\"cat_imp\", SimpleImputer(strategy=\"constant\"), catcols)])),\n",
    "#     (\"transform\", ColumnTransformer([(\"num_trns\", StandardScaler(), numcols),\n",
    "#                                           (\"cat_trns\", OneHotEncoder(drop='first'), catcols)])),                                     \n",
    "#     (\"model\", LinearRegression())\n",
    "# ])\n",
    "\n",
    "display(reg_pipe_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe_1.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(reg_pipe_1.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "reg_pipe_1[1].coef_\n",
    "# reg_pipe_1['model'].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2ffa6",
   "metadata": {},
   "source": [
    "Let's try some other combinations of the pre-processing and feature engineering steps that we have learned about this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93b339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ADDITIONAL transformer for the pipeline\n",
    "# Creating the Transformer for box-cox implementation \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def boxcoxarray(X, y=None, lmbda=None):\n",
    "    X_ = X.copy()\n",
    "    for i in range(X.shape[1]):\n",
    "        if lmbda == None:\n",
    "            X_[:,i], bc_params_ = stats.boxcox(X_[:,i], lmbda = lmbda)\n",
    "        else:\n",
    "            X_[:,i] = stats.boxcox(X_[:,i], lmbda = lmbda)\n",
    "    return X_\n",
    "\n",
    "# Creating our transformer from the pre-defined boxcoxarray callable function:\n",
    "boxtransformer = FunctionTransformer(boxcoxarray, validate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg Pipe 2 \n",
    "\n",
    "# Define column indices\n",
    "numcols = [0, 1, 2, 7]\n",
    "countcols = [3, 4, 5, 6]\n",
    "catcols = [8]\n",
    "\n",
    "# Reg Pipe 2 \n",
    "num_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "count_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_transform\", boxtransformer),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_impute\", SimpleImputer(strategy=\"constant\")),\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Overall ML pipeline inlcuding all \n",
    "reg_pipe_2 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"num_pre\", num_pre, numcols), # Applied to the columns/variables in 0, 1, 2, 7 index\n",
    "        (\"count_pre\", count_pre, countcols), # Applied to the columns/variables in 3, 4, 5, 6 index\n",
    "        (\"cat_pre\", cat_pre, catcols)])), # Applied to the columns/variables in 8 index only (ocean_proximity)\n",
    "        (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "display(reg_pipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b44b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe_2.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(reg_pipe_2.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c977962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "reg_pipe_2[1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32681c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reg Pipe 3\n",
    "from feature_engine.transformation import LogTransformer\n",
    "\n",
    "numcols = [0, 1]\n",
    "countcols = [2, 3, 4, 5, 6,7]\n",
    "catcols = [8]\n",
    "\n",
    "num_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "num2_pre = Pipeline([\n",
    "    (\"num_impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"num_transform\", LogTransformer()),\n",
    "    (\"num_scale\", StandardScaler())])\n",
    "\n",
    "cat_pre = Pipeline([\n",
    "    (\"cat_impute\", SimpleImputer(strategy=\"constant\")),\n",
    "    (\"cat_encode\", OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Overall ML pipeline inlcuding all \n",
    "reg_pipe_3 = Pipeline([\n",
    "    (\"pre_processing\", ColumnTransformer([\n",
    "        (\"num_pre\", num_pre, numcols), # Applied to the columns/variables in 0, 1 index\n",
    "        (\"num2_pre\", count_pre, countcols), # Applied to the columns/variables in 2, 3, 4, 5, 6, 7 index\n",
    "        (\"cat_pre\", cat_pre, catcols)])), # Applied to the columns/variables in 8 index only (ocean_proximity)\n",
    "        (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "display(reg_pipe_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe_3.fit(X,y)\n",
    "# Print the R squared (ranges 0 to 1, with higher values better)\n",
    "print(round(reg_pipe_3.score(X, y), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e65c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coeffcients\n",
    "reg_pipe_3[1].coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8dd4a",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 16 (CORE)\n",
    "\n",
    "Explain in words what are the differences in pre-processing and feature engineering steps in the three models above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c430de7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2daf4138",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 17 (EXTRA)\n",
    "\n",
    "Try to create your own pipeline by modifying at least one of the pre-processing and feature engineering steps above. What have you decided to change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab501d",
   "metadata": {},
   "source": [
    "# Summary <a id='sum'></a>\n",
    "\n",
    "This week we covered a lot of ground!\n",
    "\n",
    "- We've looked at some methods for pre-processing our data, cleaning and preparing it, as well as how to engineer some features. \n",
    "\n",
    "- This is by **no means a complete collection of all the methods available** as covering more would go beyond the scope of this course (for those interested in learning more, have a look though the given companion readings). \n",
    "\n",
    "- For example, we did not touch on handling text and dates/time much. These topics are quite complex and have enough materials to cover their own courses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e7817",
   "metadata": {},
   "source": [
    "# Competing the Worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Before generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are not able to edit the Metadata, then please add a markdown cell at the beginning with the names of all group members. \n",
    "\n",
    "Once that is done and you are happy with everything, you can then run the following cell \n",
    "to generate your PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937dba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to pdf mlp_week01.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7218599",
   "metadata": {},
   "source": [
    "Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. Note that:\n",
    "\n",
    ">- You don't need to finish everything, but you should have had a substantial attempt at the bulk of the material, particularly the CORE tasks. \n",
    ">- If you are having trouble generating the pdf, please ask a tutor or post on piazza. \n",
    ">- As a back option, you can simply select the print option in your browser which should generate a high-quality pdf of your complete notebook. Basically, right click → Click Print → Choose 'Save as PDF' option instead of default printing destination (Check that your all materials are visible in the print preview) → Click 'Save' to get your pdf file."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Student 1"
   },
   {
    "name": "Student 2"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "title": "MLPy Workshop 1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
